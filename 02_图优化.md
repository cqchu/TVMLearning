# NNVM图优化
在前端转化完成后, 可以调用`nnvm.compiler.build()`函数进行图以及算子阶段的优化, 其中主要使用了pass的机制, pass的注册可以参考[NNVM中的Pass注册机制](00_NNVM中的Pass注册机制), 此处主要介绍NNVM中图优化相关内容

## 初始化阶段
初始化阶段包括三个步骤: 校正Layout, 初始化shape和dtype, 初始化变量. 主要是为了之后PlanMemory等工作做准备. </br>

首先是校正Layout, 主要代码就是如下几句, 其中`layout`是形如{node_name: layout_type}的字典, 然后`set_layout_inputs()`功能就是如果`layout`有指定某个node的layout, 则更新`graph`中对应node, 若没有指定, 则保持`graph`中相关node不变. Set完成后, 再调用"CorrectLayout" pass进行校正. 具体"CorrectLayout" pass可以参考[NNVM中的Pass功能](00_NNVM中的Pass功能)
```python
graph = graph_attr.set_layout_inputs(graph, layout)
graph = graph.apply("CorrectLayout")
```

之后是shape和dtype的一些初始化, shape和dtype作为参数传进来时通常只有input的shape和dtype, 然后系统会调用`_update_shape_dtype()`得到params中各个node的shape和dtype. 然后在调用"InferShape"和"InferType"两个pass进行中间activation等shape和type的推导.
```python
shape, dtype = _update_shape_dtype(shape, dtype, params)    # 获取每个param的shape和dtype
ishape, _ = graph_util.infer_shape(graph, **shape)          # 进行shape推断, 底层调用pass
shape.update(zip(graph.index.input_names, ishape))          # 重新存储推断后shape
idtype, _ = graph_util.infer_dtype(graph, **dtype)          # 流程和shape是一样的
dtype.update(zip(graph.index.input_names, idtype))
```

最后是初始化变量, 当指定了初始化即`_all_var_init`不为空的时候, 调用`initialize_variables()`根据`_all_var_init`对图中相关node进行初始化, 具体不多赘述.


## 图优化阶段
图优化阶段也包含三个步骤: 应用优化, 预计算裁剪, 融合相邻运算.

当layout, shape, dtype都处理完毕后, 系统首先调用`optimize()`进行优化, optimize中根据不同的optimize_level选定不同的pass组合去优化, 此处暂时跳过. ------- TODO

之后是预计算裁剪, 其主要调用`precompute_prune()`函数也是利用pass进行优化, 其中调用了`_move_out_graph`函数, 该函数是`tvm.get_global_func("nnvm.graph._move_graph")`, tvm_get_global_func是调用`nnvm/src/compiler/packed_func_ext.cc`中封装的函数 此处也先跳过. --------- TODO

最后是GraphFuse, 此处也先跳过, --------- TODO



构造graph的代码，就在graph.h/cc里， 抽象出node, 而node代表一个operation，如matmul、add等等，op在nnvm表示所有和graph本身计算逻辑的一个数据结构，是计算图得以完成forward、gradient计算的的基础。op定义了基本的attr，可以对不同的op用set_attr注册不同的属性, 其实我觉得这里理解为接口会更合适，如op_attr_types.h中声明的各种函数，相当于整个graph中某个node根据你的op空出来很多槽，当选择某个node时，会填入对应的实现逻辑

infershape其实也是类似于auto diff的逻辑，每一个op会注册一个infer shape的逻辑，当对graph作一个infer shape的操作时，其实是对整个graph 作一个poster dfs，然后每一个node 去infer shape

pass中基本上都是遍历图, 完成一些操作

可以用Operator来将一些Operand给compose为新的一个Operand


    Node: 表示IndexedGraph中的一个节点，节点基本上分为Operator和Variable, 包括输入与op。
    NodeEntry: 表示图中某个节点的输入。因此对于这样一个输入，我们需要在它的数据结构内部记录它属于哪个节点的输出，以及是第几个输出。

首先描述一下MXNet对计算图基本要素的定义。对于单个Symbol以及组合出来的Symbol也就是Network来说，arguments指的是能够前向运行网络所必须的数据，例如输入数据data和label、各个层的权重，例如Convolution的kernels，以及Batch Norm的γ和β。auxiliary_states指的是一些运行时需要的，并且有必要被保存下来的特殊状态，例如Batch Norm的moving_mean和moving_var之类。另外，inputs指的是arguments和auxiliary_states的并集。最后outputs当然就是指网络的最终输出，它多数情况下都是某种Loss，不过也有可能只是计算出的数据，例如GAN的Generator。


Symbol对象仅仅包含一个vector<NodeEntry> outputs成员, 从这个output能够根据每个NodeEntry的输入逐级往前推